{"id":"paper_1702500000000","title":"Attention Is All You Need","authors":"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin","journal":"NeurIPS","year":"2017","doi":"10.5555/3295222.3295349","url":"https://arxiv.org/abs/1706.03762","keyAssumptions":"RNNs and CNNs are necessary for sequence transduction; Sequential computation limits parallelization","citation":"Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.","notes":"Foundational paper introducing the Transformer architecture. Key innovation: self-attention mechanism allows modeling dependencies without recurrence or convolution.","addedDate":"2024-01-15T10:00:00Z"}
{"id":"paper_1702500000001","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","authors":"Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova","journal":"NAACL","year":"2019","doi":"10.18653/v1/N19-1423","url":"https://arxiv.org/abs/1810.04805","keyAssumptions":"Unidirectional language models are sufficient for pre-training; Fine-tuning requires task-specific architectures","citation":"Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT (pp. 4171-4186).","notes":"Introduces bidirectional pre-training using masked language modeling. Shows that deep bidirectional representations significantly improve downstream task performance.","addedDate":"2024-01-15T10:05:00Z"}
{"id":"paper_2505_13400","title":"Robin: A multi-agent system for automating scientific discovery","authors":"Ali Essam Ghareeb, Benjamin Chang, Ludovico Mitchener, Angela Yiu, Caralyn J. Szostkiewicz, Jon M. Laurent, Muhammed T. Razzak, Andrew D. White, Michaela M. Hinks, Samuel G. Rodriques","journal":"arXiv","year":"2025","doi":"2505.13400","url":"https://arxiv.org/abs/2505.13400","keyAssumptions":"Human oversight required for complete scientific discovery; Task-specific AI tools sufficient; Domain-specific limitations in AI science","citation":"Ghareeb, A. E., Chang, B., Mitchener, L., et al. (2025). Robin: A multi-agent system for automating scientific discovery. arXiv:2505.13400.","notes":"First multi-agent system automating complete scientific discovery process. Successfully identified novel dAMD treatment (ripasudil). Integrates literature search with data analysis agents in iterative lab-in-the-loop framework.","addedDate":"2025-08-07T18:45:00Z"}
{"id":"paper_2408_06292","title":"The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery","authors":"Christopher Lu, et al.","journal":"arXiv","year":"2024","doi":"2408.06292","url":"https://arxiv.org/abs/2408.06292","keyAssumptions":"Human creativity required for scientific discovery; Scientific writing requires deep domain understanding; Peer review needs human judgment","citation":"Lu, C., et al. (2024). The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. arXiv:2408.06292.","notes":"First comprehensive framework for fully automatic scientific discovery. Generates research ideas, executes experiments, writes papers, and runs simulated review. Generated papers exceed conference acceptance thresholds at $15/paper.","addedDate":"2025-08-07T18:45:00Z"}
{"id":"paper_2504_08066","title":"The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search","authors":"Yutaro Yamada, et al.","journal":"arXiv","year":"2025","doi":"2504.08066","url":"https://arxiv.org/abs/2504.08066","keyAssumptions":"Template dependency necessary for autonomous research; Limited domain generalization; Simple experiment management sufficient","citation":"Yamada, Y., et al. (2025). The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search. arXiv:2504.08066.","notes":"Eliminates template dependency through agentic tree search. First AI-generated paper accepted at peer-reviewed workshop. Progressive experiment management with VLM-enhanced figures.","addedDate":"2025-08-07T18:45:00Z"}
{"id":"paper_2503_24047","title":"Towards Scientific Intelligence: A Survey of LLM-based Scientific Agents","authors":"Shuo Ren, et al.","journal":"arXiv","year":"2025","doi":"2503.24047","url":"https://arxiv.org/abs/2503.24047","keyAssumptions":"General agents sufficient for scientific tasks; Domain independence adequate; Simple tool integration works; Standard AI benchmarks measure scientific capability","citation":"Ren, S., et al. (2025). Towards Scientific Intelligence: A Survey of LLM-based Scientific Agents. arXiv:2503.24047.","notes":"Comprehensive survey establishing scientific intelligence as distinct field. Shows need for specialized architectures with domain knowledge, advanced tools, and robust validation mechanisms.","addedDate":"2025-08-07T18:45:00Z"}
{"id":"paper_2505_19897","title":"ScienceBoard: Evaluating Multimodal Autonomous Agents for Scientific Research","authors":"Qiushi Sun, et al.","journal":"arXiv","year":"2025","doi":"2505.19897","url":"https://arxiv.org/abs/2505.19897","keyAssumptions":"Simplified evaluation sufficient; Single-domain focus adequate; Interface limitations acceptable; Static benchmarks work","citation":"Sun, Q., et al. (2025). ScienceBoard: Evaluating Multimodal Autonomous Agents for Scientific Research. arXiv:2505.19897.","notes":"Comprehensive evaluation environment with 169 expert-validated tasks across biochemistry, astronomy, geoinformatics. Reveals performance gaps in current agents for realistic scientific workflows.","addedDate":"2025-08-07T18:45:00Z"}